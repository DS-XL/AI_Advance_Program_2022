{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5306480-b05c-46e6-91f9-f03429343e5f",
   "metadata": {},
   "source": [
    "# ðŸ¤– Prototyping models for Meet Fresh recommender - Collaborative Filtering\n",
    "For Meet Fresh product solution POC, we prototype multiple models that could be used for providing recommendations on different levels -\n",
    "\n",
    "- Recommending ingredients based on customer ingredient ratings\n",
    "- Recommending products based on ingredient selections\n",
    "\n",
    "Here we outline the process of building a POC for collaborative filtering using ingredent ratings.\n",
    "\n",
    "References:\n",
    "- [Prototyping a Recommender System Step by Step Part 2: Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering](https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1)\n",
    "- [Running ALS on MovieLens (PySpark)](https://github.com/microsoft/recommenders/blob/main/examples/00_quick_start/als_movielens.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d5706-fb6a-4a03-a7a1-6c8aa6a0c6bf",
   "metadata": {},
   "source": [
    "### Model 1: Collaborative Filtering Using Ingredient Ratings\n",
    "This CF model uses only customer ratings (1-3) for ingredient items, outputs predicted ratings for the ingredients that have not been rated on by the customer. The assumption goes that customer only expresses explicit preference from providing rating, but not providing a rating does not mean they are NOT interested (as they might just not know that they like the item).\n",
    "\n",
    "Based on predicted ratings for unrated ingredients, recommendations could be by sorting on predicted ratings.\n",
    "\n",
    "In practical implementation, we could only use this approach when we have accumulated significant amount of ratings data. To address cold start problem, we could utilize content-based filtering and other approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8c8cd-5fa3-415f-a9df-815092f05a5c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yyaml (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting recommenders\n",
      "  Using cached recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.21.6)\n",
      "Requirement already satisfied: pandas<2,>1.0.3 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.3.5)\n",
      "Requirement already satisfied: scipy<2,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.7.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.31.1 in /opt/conda/lib/python3.7/site-packages (from recommenders) (4.63.0)\n",
      "Requirement already satisfied: matplotlib<4,>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from recommenders) (3.5.3)\n",
      "Requirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.0.2)\n",
      "Requirement already satisfied: numba<1,>=0.38.1 in /opt/conda/lib/python3.7/site-packages (from recommenders) (0.56.4)\n",
      "Collecting lightfm<2,>=1.15 (from recommenders)\n",
      "  Using cached lightfm-1.17-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting lightgbm>=2.2.1 (from recommenders)\n",
      "  Using cached lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting memory-profiler<1,>=0.54.0 (from recommenders)\n",
      "  Using cached memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Collecting nltk<4,>=3.4 (from recommenders)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: seaborn<1,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from recommenders) (0.12.2)\n",
      "Collecting transformers<5,>=2.5.0 (from recommenders)\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: bottleneck<2,>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.3.5)\n",
      "Collecting category-encoders<2,>=1.3.0 (from recommenders)\n",
      "  Using cached category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting jinja2<3.1,>=2 (from recommenders)\n",
      "  Using cached Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting pyyaml<6,>=5.4.1 (from recommenders)\n",
      "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from recommenders) (2.31.0)\n",
      "Collecting cornac<2,>=1.1.2 (from recommenders)\n",
      "  Using cached cornac-1.15.4-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from recommenders) (1.3.4)\n",
      "Collecting pandera[strategies]>=0.6.5 (from recommenders)\n",
      "  Using cached pandera-0.15.1-py3-none-any.whl (152 kB)\n",
      "Collecting scikit-surprise>=1.0.6 (from recommenders)\n",
      "  Using cached scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders<2,>=1.3.0->recommenders) (0.13.5)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.3)\n",
      "Collecting powerlaw (from cornac<2,>=1.1.2->recommenders)\n",
      "  Using cached powerlaw-1.5-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2<3.1,>=2->recommenders) (2.1.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm>=2.2.1->recommenders) (0.40.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.9.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders) (2023.6.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba<1,>=0.38.1->recommenders) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba<1,>=0.38.1->recommenders) (67.7.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from numba<1,>=0.38.1->recommenders) (4.11.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2,>1.0.3->recommenders) (2023.3)\n",
      "Requirement already satisfied: multimethod in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders) (1.9.1)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders) (1.10.9)\n",
      "Collecting typeguard>=3.0.2 (from pandera[strategies]>=0.6.5->recommenders)\n",
      "  Using cached typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting typing-inspect>=0.6.0 (from pandera[strategies]>=0.6.5->recommenders)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders) (4.6.3)\n",
      "Collecting hypothesis>=5.41.1 (from pandera[strategies]>=0.6.5->recommenders)\n",
      "  Using cached hypothesis-6.79.4-py3-none-any.whl (417 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from retrying>=1.3.3->recommenders) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<5,>=2.5.0->recommenders)\n",
      "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders) (0.3.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<5,>=2.5.0->recommenders) (2023.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->numba<1,>=0.38.1->recommenders) (3.15.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.6.0->pandera[strategies]>=0.6.5->recommenders)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: mpmath in /opt/conda/lib/python3.7/site-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yyaml (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyyaml, mypy-extensions, memory-profiler, jinja2, typing-inspect, typeguard, scikit-surprise, huggingface-hub, transformers, powerlaw, pandera, nltk, lightgbm, lightfm, hypothesis, cornac, category-encoders, recommenders\n",
      "\u001b[33m  WARNING: The script mprof is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script surprise is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script hypothesis is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.2.0 requires typeguard<3,>=2.13.2, but you have typeguard 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed category-encoders-1.3.0 cornac-1.15.4 huggingface-hub-0.15.1 hypothesis-6.79.4 jinja2-3.0.3 lightfm-1.17 lightgbm-3.3.5 memory-profiler-0.61.0 mypy-extensions-1.0.0 nltk-3.8.1 pandera-0.15.1 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.1 scikit-surprise-1.1.1 transformers-4.30.2 typeguard-4.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install recommenders --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b033cf-413d-4346-9694-88cea7b679b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b7b1d-b63c-4a3b-90b6-1476d7a0d60e",
   "metadata": {},
   "source": [
    "#### Step 1 - Data pre-processing for running ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114e8b4-fa42-485b-8835-941c3165718b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6040bb5affc64f35bf0da5bc9ec9f341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97c4a3e354c4a72b87b37931f057fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery ratings_orig_df\n",
    "SELECT * FROM `dsxl-ai-advanced-program.meetfresh.ft_customer_ingredient_ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583963e6-8623-4d4c-a2f2-504e5603d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500029 entries, 0 to 500028\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   customer_id       500029 non-null  object\n",
      " 1   ingredient_id     500029 non-null  object\n",
      " 2   ingredient_name   500029 non-null  object\n",
      " 3   meetfresh_rating  500029 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.3+ MB\n"
     ]
    }
   ],
   "source": [
    "ratings_orig_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14eeb8-d86f-4370-bbc1-410de32ba4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>ingredient_id</th>\n",
       "      <th>ingredient_name</th>\n",
       "      <th>meetfresh_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156294</td>\n",
       "      <td>B15</td>\n",
       "      <td>Tofu Pudding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206256</td>\n",
       "      <td>B15</td>\n",
       "      <td>Tofu Pudding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248808</td>\n",
       "      <td>B15</td>\n",
       "      <td>Tofu Pudding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141890</td>\n",
       "      <td>B15</td>\n",
       "      <td>Tofu Pudding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116806</td>\n",
       "      <td>B15</td>\n",
       "      <td>Tofu Pudding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500024</th>\n",
       "      <td>35223</td>\n",
       "      <td>UNK1002</td>\n",
       "      <td>Almond Flakes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500025</th>\n",
       "      <td>98035</td>\n",
       "      <td>UNK1002</td>\n",
       "      <td>Almond Flakes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500026</th>\n",
       "      <td>11387</td>\n",
       "      <td>UNK1002</td>\n",
       "      <td>Almond Flakes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500027</th>\n",
       "      <td>82186</td>\n",
       "      <td>UNK1002</td>\n",
       "      <td>Almond Flakes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500028</th>\n",
       "      <td>267772</td>\n",
       "      <td>UNK1002</td>\n",
       "      <td>Almond Flakes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500029 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id ingredient_id ingredient_name meetfresh_rating\n",
       "0           156294           B15    Tofu Pudding              1.0\n",
       "1           206256           B15    Tofu Pudding              1.0\n",
       "2           248808           B15    Tofu Pudding              1.0\n",
       "3           141890           B15    Tofu Pudding              1.0\n",
       "4           116806           B15    Tofu Pudding              1.0\n",
       "...            ...           ...             ...              ...\n",
       "500024       35223       UNK1002   Almond Flakes              3.0\n",
       "500025       98035       UNK1002   Almond Flakes              3.0\n",
       "500026       11387       UNK1002   Almond Flakes              3.0\n",
       "500027       82186       UNK1002   Almond Flakes              3.0\n",
       "500028      267772       UNK1002   Almond Flakes              3.0\n",
       "\n",
       "[500029 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b88cef-649c-4a08-a8bb-2ef6abf8e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-based API for ALS currently only supports integers for user and item ids\n",
    "# map ingredient_id to 8-digit numeric id values\n",
    "import random\n",
    "random.seed(101)\n",
    "\n",
    "original_ids = ratings_orig_df['ingredient_id'].unique()\n",
    "\n",
    "while True:\n",
    "    new_ids = {id_: random.randint(10_000_000, 99_999_999) for id_ in original_ids}\n",
    "    if len(set(new_ids.values())) == len(original_ids):\n",
    "        # all the generated id's were unique\n",
    "        break\n",
    "    # otherwise this will repeat until they are\n",
    "\n",
    "ratings_orig_df['ingredient_id_int'] = ratings_orig_df['ingredient_id'].map(new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0727eb5-7329-4918-9900-404e1d9e131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500029 entries, 0 to 500028\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   customer_id        500029 non-null  int64  \n",
      " 1   ingredient_id      500029 non-null  object \n",
      " 2   ingredient_name    500029 non-null  object \n",
      " 3   meetfresh_rating   500029 non-null  float64\n",
      " 4   ingredient_id_int  500029 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# customer_id should be integer, meetfresh_rating should be float\n",
    "ratings_orig_df['customer_id'] = ratings_orig_df['customer_id'].astype(int)\n",
    "ratings_orig_df['meetfresh_rating'] = ratings_orig_df['meetfresh_rating'].astype(float)\n",
    "ratings_orig_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c17cfb-97be-4c11-8499-e4c5b6cd184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- ingredient_id_int: integer (nullable = true)\n",
      " |-- meetfresh_rating: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 19:58:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 0 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------------+\n",
      "|customer_id|ingredient_id_int|meetfresh_rating|\n",
      "+-----------+-----------------+----------------+\n",
      "|     156294|         88000918|             1.0|\n",
      "|     206256|         88000918|             1.0|\n",
      "|     248808|         88000918|             1.0|\n",
      "|     141890|         88000918|             1.0|\n",
      "|     116806|         88000918|             1.0|\n",
      "|     197318|         88000918|             1.0|\n",
      "|     133389|         88000918|             1.0|\n",
      "|       9423|         88000918|             1.0|\n",
      "|     265892|         88000918|             1.0|\n",
      "|     130654|         88000918|             1.0|\n",
      "|     140511|         88000918|             1.0|\n",
      "|      95722|         88000918|             1.0|\n",
      "|     255165|         88000918|             1.0|\n",
      "|      24086|         88000918|             1.0|\n",
      "|     128695|         88000918|             1.0|\n",
      "|     151125|         88000918|             1.0|\n",
      "|     154479|         88000918|             1.0|\n",
      "|      84830|         88000918|             1.0|\n",
      "|     260008|         88000918|             1.0|\n",
      "|     127455|         88000918|             1.0|\n",
      "+-----------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# turn dataframes into spark df\n",
    "ratings_df = ratings_orig_df[['customer_id', 'ingredient_id_int', 'meetfresh_rating']]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('customer_id', IntegerType()),\n",
    "        StructField('ingredient_id_int', IntegerType()),\n",
    "        StructField('meetfresh_rating', FloatType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "sparkDF=spark.createDataFrame(ratings_df, schema=schema) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc94f72-b4a6-4f24-bf53-67af41459b7d",
   "metadata": {},
   "source": [
    "#### Step 2 - ALS model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c82f12-6002-4713-b898-e40a7ef61543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some global parameter values\n",
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"customer_id\"\n",
    "COL_ITEM = \"ingredient_id_int\"\n",
    "COL_RATING = \"meetfresh_rating\"\n",
    "# COL_TIMESTAMP = \"Timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3242713-9368-4728-ac8f-c0c19fa145e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 19:58:58 WARN org.apache.spark.scheduler.TaskSetManager: Stage 1 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 374966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 19:59:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 3 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 3:==============>                                            (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N test 125063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# split data into train and validation sets\n",
    "train_data, validation_data = spark_random_split(sparkDF, ratio=0.75, seed=123)\n",
    "print (\"N train\", train_data.cache().count())\n",
    "print (\"N test\", validation_data.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcc260-fa94-4b57-a6df-af8e80813262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 19:59:15 WARN org.apache.spark.scheduler.TaskSetManager: Stage 5 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 19:59:15 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 19:59:20 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/30 19:59:20 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/06/30 19:59:21 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/06/30 19:59:21 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train model with some default hyperparameters\n",
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING,\n",
    "}\n",
    "\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")\n",
    "\n",
    "model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa5524-380e-40ee-a010-14d26e6b4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 19:59:55 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'customer_id#0 = customer_id#0'. Perhaps you need to use aliases.\n",
      "23/06/30 19:59:55 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'ingredient_id_int#1 = ingredient_id_int#1'. Perhaps you need to use aliases.\n",
      "23/06/30 19:59:56 WARN org.apache.spark.scheduler.TaskSetManager: Stage 81 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 19:59:58 WARN org.apache.spark.scheduler.TaskSetManager: Stage 83 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 19:59:58 WARN org.apache.spark.scheduler.TaskSetManager: Stage 84 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 123:====================================================>(199 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------+\n",
      "|customer_id|ingredient_id_int|prediction|\n",
      "+-----------+-----------------+----------+\n",
      "|         11|         38792785| 1.4237518|\n",
      "|         11|         54162664| 1.3280461|\n",
      "|         14|         72698292| 1.1512083|\n",
      "|         35|         72698292| 1.1512083|\n",
      "|         80|         75157816| 1.4560395|\n",
      "|         84|         38504727| 1.5660988|\n",
      "|        111|         54162664|  1.363963|\n",
      "|        129|         19442953| 2.4012413|\n",
      "|        170|         90804821| 1.4121011|\n",
      "|        185|         90804821| 0.5803766|\n",
      "|        210|         35942492| 0.6856568|\n",
      "|        210|         90804821|0.76456535|\n",
      "|        218|         54162664| 1.1853198|\n",
      "|        221|         44257992| 1.6205008|\n",
      "|        242|         19442953| 0.5109888|\n",
      "|        242|         38792785| 0.5643073|\n",
      "|        248|         99049522| 1.3198466|\n",
      "|        260|         77581358|0.84048474|\n",
      "|        295|         90804821|0.75884974|\n",
      "|        325|         48724825| 1.5827569|\n",
      "+-----------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# recommending ingredients already rated by customers does not make sense and we need to remove those already rated\n",
    "\n",
    "# get the cross join of all user-item pairs and score them\n",
    "users = train_data.select(COL_USER).distinct()\n",
    "items = train_data.select(COL_ITEM).distinct()\n",
    "user_item = users.crossJoin(items)\n",
    "dfs_pred = model.transform(user_item)\n",
    "\n",
    "# remove rated items\n",
    "dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "    train_data.alias(\"train\"),\n",
    "    (dfs_pred[COL_USER] == train_data[COL_USER]) & (dfs_pred[COL_ITEM] == train_data[COL_ITEM]),\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
    "    .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "top_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ac0c6-57b2-4a92-b9d9-83456a278754",
   "metadata": {},
   "source": [
    "#### Step 3 - ALS model rating predictions evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f8165-dfb8-4da0-99f9-33a5ceea7c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:01:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 160 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------------+----------+\n",
      "|customer_id|ingredient_id_int|meetfresh_rating|prediction|\n",
      "+-----------+-----------------+----------------+----------+\n",
      "|     108560|         21927066|             1.0| 1.3774933|\n",
      "|        392|         21927066|             2.0|  1.022362|\n",
      "|     180457|         21927066|             2.0| 1.6426476|\n",
      "|     183122|         21927066|             2.0| 1.6831236|\n",
      "|     267772|         21927066|             1.0| 1.3448393|\n",
      "|      19131|         21927066|             2.0| 1.5322832|\n",
      "|     156123|         21927066|             1.0| 1.0642309|\n",
      "|     166547|         21927066|             1.0|   1.06981|\n",
      "|     179412|         21927066|             2.0| 1.1142436|\n",
      "|      45027|         21927066|             1.0| 0.8961086|\n",
      "|      66594|         21927066|             1.0| 0.5315274|\n",
      "|     125475|         21927066|             1.0| 1.1228914|\n",
      "|     216362|         21927066|             1.0| 2.1285985|\n",
      "|     257766|         21927066|             1.0| 0.6986665|\n",
      "|     111531|         21927066|             1.0|  1.185076|\n",
      "|     166964|         21927066|             2.0| 1.7185295|\n",
      "|      26921|         21927066|             2.0| 2.2789867|\n",
      "|      53872|         21927066|             2.0| 0.9004349|\n",
      "|     101678|         21927066|             1.0| 1.0756519|\n",
      "|     171854|         21927066|             2.0| 1.7331284|\n",
      "+-----------+-----------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predicted ratings on validation data\n",
    "prediction = model.transform(validation_data)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a0f23-c6ad-4227-8632-afdf60c3e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:05:09 WARN org.apache.spark.scheduler.TaskSetManager: Stage 242 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:05:14 WARN org.apache.spark.scheduler.TaskSetManager: Stage 323 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:05:17 WARN org.apache.spark.scheduler.TaskSetManager: Stage 363 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t0.900810\n",
      "MAE:\t0.710196\n",
      "Explained variance:\t-0.246854\n",
      "R squared:\t-0.482952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:05:19 WARN org.apache.spark.scheduler.TaskSetManager: Stage 404 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(validation_data, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c4ccf-e1df-468c-a91b-d4f716b2a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:11:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 406 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 442 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 448 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 484 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:48 WARN org.apache.spark.scheduler.TaskSetManager: Stage 489 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:48 WARN org.apache.spark.scheduler.TaskSetManager: Stage 524 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 530 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:11:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 565 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 569:====================================================>(197 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t0.343862\n",
      "MAE:\t0.226626\n",
      "Explained variance:\t0.791472\n",
      "R squared:\t0.786017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# what about training data\n",
    "prediction_train = model.transform(train_data)\n",
    "rating_eval_train = SparkRatingEvaluation(train_data, prediction_train, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval_train.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval_train.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval_train.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval_train.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b5823-854e-4215-aceb-878a09ee5fdf",
   "metadata": {},
   "source": [
    "Model performs much better on training data than on validation data -> serious sign of high variance problem and model is overfitting on training data. In next step we find a way to conduct hyperparameter tuning and adjust regularization to account for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463391c7-118b-4e40-9d3d-49e5e5027aed",
   "metadata": {},
   "source": [
    "#### Step 4 - ALS model hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88d49e-c055-4dca-ac2a-c77e7078e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of validation data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark DF with columns ['customer_id', 'ingredient_id_int', 'meetfresh_rating']\n",
    "    \n",
    "    validation_data: spark DF with columns ['customer_id', 'ingredient_id_int', 'meetfresh_rating']\n",
    "    \n",
    "    maxIter: int, max number of learning iterations\n",
    "    \n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING}\n",
    "    \n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = ALS(\n",
    "                        rank=rank,\n",
    "                        maxIter=15,\n",
    "                        implicitPrefs=False,\n",
    "                        regParam=reg,\n",
    "                        coldStartStrategy='drop',\n",
    "                        nonnegative=False,\n",
    "                        seed=42,\n",
    "                        **header\n",
    "                    )\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            prediction = model.transform(validation_data)\n",
    "            rating_eval = SparkRatingEvaluation(validation_data, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "            rmse = rating_eval.rmse()\n",
    "            \n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            \n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350cbbc-e0d0-45cd-9be4-59c94ceb4005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:50:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6772 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:50:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6773 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:50:52 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6848 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:50:52 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6884 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:50:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6890 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:50:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6925 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 latent factors and regularization = 0.1: validation RMSE is 0.8371197275451394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:51:05 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6931 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:51:05 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6932 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:52:23 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7007 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:52:23 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7043 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:52:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7049 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:52:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7085 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 latent factors and regularization = 0.1: validation RMSE is 0.8380459724351232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:52:35 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7090 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:52:35 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7091 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:54:26 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7166 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:54:26 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7203 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:54:29 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7208 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:54:30 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7244 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 7247:===================================================>(198 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 latent factors and regularization = 0.1: validation RMSE is 0.8378188338922786\n",
      "\n",
      "The best model has 50 latent factors and regularization = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALS_5fc1ad3ca6ce"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regParams = [0.1]\n",
    "ranks = [50,60,70]\n",
    "\n",
    "tune_ALS(train_data, validation_data, 15, regParams, ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0182ff-1a84-4d75-ab4e-086b249683bc",
   "metadata": {},
   "source": [
    "After tuning, seems that the best hyperparameter values to use are regParams = 0.1 and ranks = 50\n",
    "Due to the nature of ratings data used for this exercise, it is expected to have less than ideal performance on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a35510-9db6-44d4-8cb0-c0a6c521f2cf",
   "metadata": {},
   "source": [
    "#### Step 5 - Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0823c-e58a-4825-b2cd-c006e2f7ffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 20:55:47 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7249 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:55:47 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7250 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "23/06/30 20:56:40 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7360 contains a task of very large size (2908 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------------+----------+\n",
      "|customer_id|ingredient_id_int|meetfresh_rating|prediction|\n",
      "+-----------+-----------------+----------------+----------+\n",
      "|     108560|         21927066|             1.0| 1.7628738|\n",
      "|        392|         21927066|             2.0| 1.3824956|\n",
      "|     180457|         21927066|             2.0| 1.2576532|\n",
      "|     183122|         21927066|             2.0|  1.451288|\n",
      "|     267772|         21927066|             1.0| 1.5627604|\n",
      "|      19131|         21927066|             2.0| 1.5108426|\n",
      "|     156123|         21927066|             1.0| 0.9783255|\n",
      "|     166547|         21927066|             1.0| 1.0476387|\n",
      "|     179412|         21927066|             2.0| 1.1452771|\n",
      "|      45027|         21927066|             1.0| 0.9671886|\n",
      "|      66594|         21927066|             1.0|0.61436623|\n",
      "|     125475|         21927066|             1.0| 1.2123389|\n",
      "|     216362|         21927066|             1.0| 1.2300282|\n",
      "|     257766|         21927066|             1.0| 0.7539046|\n",
      "|     111531|         21927066|             1.0| 1.5067697|\n",
      "|     166964|         21927066|             2.0| 1.6818295|\n",
      "|      26921|         21927066|             2.0| 2.0079088|\n",
      "|      53872|         21927066|             2.0|  1.291296|\n",
      "|     101678|         21927066|             1.0| 1.2667847|\n",
      "|     171854|         21927066|             2.0| 1.1646858|\n",
      "+-----------+-----------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "        rank=50,\n",
    "        maxIter=15,\n",
    "        implicitPrefs=False,\n",
    "        regParam=0.1,\n",
    "        coldStartStrategy='drop',\n",
    "        nonnegative=False,\n",
    "        seed=42,\n",
    "        **header\n",
    "    )\n",
    "\n",
    "model = als.fit(train_data)\n",
    "prediction = model.transform(validation_data)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdbf0d-01ed-44b4-b402-a21af8309076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>ingredient_id_int</th>\n",
       "      <th>meetfresh_rating</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108560</td>\n",
       "      <td>21927066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.762874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392</td>\n",
       "      <td>21927066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.382496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180457</td>\n",
       "      <td>21927066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.257653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183122</td>\n",
       "      <td>21927066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.451288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>267772</td>\n",
       "      <td>21927066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.562760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105378</th>\n",
       "      <td>250609</td>\n",
       "      <td>82373836</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.760008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105379</th>\n",
       "      <td>252031</td>\n",
       "      <td>82373836</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.257197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105380</th>\n",
       "      <td>255183</td>\n",
       "      <td>82373836</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.405112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105381</th>\n",
       "      <td>259618</td>\n",
       "      <td>82373836</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.973809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105382</th>\n",
       "      <td>264766</td>\n",
       "      <td>82373836</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.440281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105383 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id  ingredient_id_int  meetfresh_rating  prediction\n",
       "0            108560           21927066               1.0    1.762874\n",
       "1               392           21927066               2.0    1.382496\n",
       "2            180457           21927066               2.0    1.257653\n",
       "3            183122           21927066               2.0    1.451288\n",
       "4            267772           21927066               1.0    1.562760\n",
       "...             ...                ...               ...         ...\n",
       "105378       250609           82373836               2.0    1.760008\n",
       "105379       252031           82373836               2.0    1.257197\n",
       "105380       255183           82373836               2.0    2.405112\n",
       "105381       259618           82373836               2.0    1.973809\n",
       "105382       264766           82373836               3.0    1.440281\n",
       "\n",
       "[105383 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_pred_df = prediction.toPandas()\n",
    "validation_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb559d2-a3cb-4d62-b884-a55044ee4945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
